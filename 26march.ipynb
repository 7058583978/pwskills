{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170a37b-390e-48f8-a9c5-ba5adab7d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aac551-cd02-4125-82ff-6c0792cd12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple linear regression is a statistical method that allows us to model the relationship between\n",
    "two variables by fitting a linear equation to the data. In simple linear regression, there is only\n",
    "one independent variable (predictor variable) and one dependent variable (response variable).\n",
    "The goal is to find the best-fitting straight line that summarizes the relationship between the two variables.\n",
    "An example of simple linear regression is modeling the relationship between the price of a house\n",
    "dependent variable and its size (independent variable).\n",
    "\n",
    "On the other hand, multiple linear regression is a statistical method that allows us to model the\n",
    "relationship between a dependent variable and two or more independent variables. Multiple linear \n",
    "regression attempts to find the best-fitting linear equation with multiple independent variables\n",
    "that explains the variation in the dependent variable. An example of multiple linear regression \n",
    "is modeling the relationship between the sales of a product (dependent variable) and its price,\n",
    "advertising expenditure, and product quality (independent variables).\n",
    "\n",
    "In summary, the key difference between simple and multiple linear regression is the number of\n",
    "independent variables used to model the relationship with the dependent variable. Simple linear \n",
    "regression involves one independent variable, while multiple linear regression involves two or \n",
    "more independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135034b-ac4f-4493-92e1-dcbb721d704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11990b10-9ae9-4b42-b5d9-304f1c0e037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linearity: The relationship between the dependent variable and each independent variable is linear.\n",
    "\n",
    "Independence: The observations are independent of each other.\n",
    "\n",
    "Homoscedasticity: The variance of the errors (residuals) is constant across all levels of the \n",
    "independent variable(s).\n",
    "\n",
    "Normality: The errors (residuals) follow a normal distribution.\n",
    "\n",
    "No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can use various diagnostic plots \n",
    "and statistical tests. Some common methods to check these assumptions are:\n",
    "\n",
    "Scatter plots: You can plot the dependent variable against each independent variable to check \n",
    "if the relationship is linear.\n",
    "\n",
    "Residual plots: You can plot the residuals against the predicted values to check if the variance \n",
    "of the residuals is constant and if there are any patterns in the residuals.\n",
    "\n",
    "Normality plots: You can plot the residuals on a normal probability plot to check if the residuals \n",
    "follow a normal distribution.\n",
    "\n",
    "Cook's distance: Cook's distance is a measure of the influence of each observation on the regression coefficients.\n",
    "Observations with a large Cook's distance may have a disproportionate impact on the regression results.\n",
    "\n",
    "Variance Inflation Factor (VIF): The VIF is a measure of how much the variance of the estimated regression \n",
    "coefficients is increased due to multicollinearity between the independent variables. \n",
    "A high VIF value indicates a problem with multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bea357-94ff-4ab2-9080-2e12f5176609",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b304b47-55a6-4ea7-a170-697486e99ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "in a linear regression model, the slope and intercept represent the relationship between the independent and \n",
    "dependent variables. The intercept represents the value of the dependent variable when the independent \n",
    "variable is equal to zero. The slope represents the change in the dependent variable for a one-unit increase \n",
    "in the independent variable.\n",
    "\n",
    "For example, let's say we want to model the relationship between the number of hours studied and the score \n",
    "obtained in an exam. We have collected data from a sample of students and fitted a linear regression model. \n",
    "The model is:\n",
    "\n",
    "Score = 30 + 5 * Hours_studied\n",
    "\n",
    "In this model, the intercept is 30, which represents the expected score for a student who has not studied at all. \n",
    "The slope is 5, which means that for every additional hour of study, we can expect a 5-point increase in the score.\n",
    "\n",
    "Therefore, we can interpret the intercept and slope as follows:\n",
    "\n",
    "Intercept: The expected value of the dependent variable when the independent variable is equal to zero.\n",
    "In our example, the intercept of 30 means that we can expect a score of 30 for a student who has not studied at all.\n",
    "\n",
    "\n",
    "Slope: The change in the dependent variable for a one-unit increase in the independent variable.\n",
    "In our example, the slope of 5 means that we can expect a 5-point increase in the score for every \n",
    "additional hour of study.\n",
    "\n",
    "In summary, the intercept and slope in a linear regression model provide valuable information about\n",
    "the relationship between the independent and dependent variables. They can help us make predictions\n",
    "and understand the effect of the independent variable on the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5688a-51b0-4837-b64c-aa75eff9db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d6d33-f620-434e-8d04-b215d9c2f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient descent is a numerical optimization algorithm that is commonly used in machine\n",
    "learning for finding the parameters of a model that minimize a loss function. In machine \n",
    "learning, the goal is often to find the set of parameters of a model that will produce the\n",
    "best predictions on a given dataset. Gradient descent is one of the most popular methods used\n",
    "to achieve this goal.\n",
    "\n",
    "Gradient descent works by iteratively adjusting the parameters of the model in the direction of\n",
    "the negative gradient of the loss function. The negative gradient indicates the direction of steepest descent, \n",
    "which is the direction that leads to the minimum of the loss function. By following the negative gradient,\n",
    "gradient descent algorithm tries to reach the minimum point of the loss function.\n",
    "\n",
    "The algorithm starts with an initial guess of the parameter values and then iteratively updates the values\n",
    "by taking small steps in the direction of the negative gradient. The size of the steps is determined by a\n",
    "learning rate, which controls how quickly the algorithm converges to the minimum point. A smaller learning \n",
    "rate means that the algorithm takes smaller steps, which can be slower but more accurate. A larger learning rate\n",
    "means that the algorithm takes larger steps, which can be faster but may lead to overshooting the minimum point.\n",
    "\n",
    "Gradient descent is used in machine learning to update the parameters of a model during training. \n",
    "During training, the model is fed with input data, and the output is compared to the actual output to calculate\n",
    "the loss function. The parameters are then updated using gradient descent to minimize the loss function.\n",
    "This process is repeated iteratively until the model converges to a point where the loss function is minimized\n",
    "and the model produces the best possible predictions on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c075ca-9f55-4445-9242-c569fe9f2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a2c90-cede-46b8-b4fa-aba266da21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple linear regression is a statistical method that allows us to model the\n",
    "relationship between a dependent variable and two or more independent variables. Multiple linear \n",
    "regression attempts to find the best-fitting linear equation with multiple independent variables\n",
    "that explains the variation in the dependent variable. An example of multiple linear regression \n",
    "is modeling the relationship between the sales of a product (dependent variable) and its price,\n",
    "advertising expenditure, and product quality (independent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb883a6f-ab3b-4b8e-8d7f-55b2d1862fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98179d-d4a5-4cc5-8719-7a12b76f10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity in multiple linear regression refers to the situation where two or more predictor \n",
    "variables are highly correlated with each other. This high correlation between the predictor variables can cause problems in the model, such as making it difficult to estimate the effect of each variable on the outcome variable, and increasing the variance of the estimated coefficients.\n",
    "\n",
    "Detecting multicollinearity is important before fitting a multiple linear regression model.\n",
    "There are several ways to detect multicollinearity in a dataset, including:\n",
    "\n",
    "Correlation matrix: A correlation matrix can be used to check the correlation between each pair\n",
    "of predictor variables. High correlation values (e.g., above 0.7 or 0.8) indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): VIF measures the degree of multicollinearity between each predictor \n",
    "variable and all other predictor variables. A VIF value greater than 5 or 10 is often used as a threshold\n",
    "to indicate multicollinearity.\n",
    "\n",
    "\n",
    "Once multicollinearity is detected, there are several ways to address this issue:\n",
    "\n",
    "Drop one or more highly correlated variables: This approach involves removing one or more highly correlated \n",
    "predictor variables from the model to reduce the degree of multicollinearity.\n",
    "\n",
    "Combine highly correlated variables: This approach involves combining two or more highly correlated predictor \n",
    "variables into a single variable.\n",
    "\n",
    "Ridge regression: Ridge regression is a regularization technique that adds a penalty term to the coefficients \n",
    "of the model to reduce the effect of multicollinearity.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a technique that transforms the predictor variables into a new set \n",
    "of uncorrelated variables. These new variables can then be used as input for the regression model, reducing the\n",
    "effect of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1bc56-d152-491d-bdae-e1caa31081e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf49ec-37e2-4c71-b805-77605b468c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial regression is a form of regression analysis in which the relationship between \n",
    "the predictor variable x and the outcome variable y is modeled as an nth degree polynomial function. \n",
    "This model is more flexible than linear regression as it can capture non-linear relationships between\n",
    "the predictor variable and the outcome variable.\n",
    "\n",
    "Linear regression, on the other hand, models the relationship between x and y as a linear function. \n",
    "It assumes that the relationship between x and y is a straight line and is limited to only capturing\n",
    "linear relationships.\n",
    "\n",
    "In a polynomial regression model, the equation can be written as follows:\n",
    "\n",
    "y = b0 + b1x + b2x^2 + ... + bn*x^n\n",
    "\n",
    "Here, x is the predictor variable, y is the outcome variable, b0, b1, b2, ..., bn are the coefficients \n",
    "of the polynomial terms, and n is the degree of the polynomial.\n",
    "\n",
    "Polynomial regression models can be used to fit curves to data that cannot be modeled accurately with linear\n",
    "regression. For example, if the relationship between the predictor variable and the outcome variable is \n",
    "a U-shaped curve, linear regression would not be able to capture this relationship accurately. In such cases\n",
    "a polynomial regression model with a quadratic term (i.e., n=2) can be used to capture the U-shaped curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05abd9e-7eff-45ac-aefa-9dd1c2f9a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c8c528-849b-4780-acaf-35ea5ed1b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of polynomial regression compared to linear regression:\n",
    "\n",
    "Flexibility: Polynomial regression can capture non-linear relationships between the predictor variable \n",
    "and the outcome variable, making it more flexible than linear regression.\n",
    "\n",
    "Improved model fit: Polynomial regression can result in a better fit to the data than linear regression, \n",
    "especially when the relationship between the predictor variable and the outcome variable is non-linear.\n",
    "\n",
    "Disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "Overfitting: Polynomial regression models with a high degree of polynomial terms can lead to overfitting,\n",
    "where the model fits the noise in the data rather than the underlying relationship.\n",
    "\n",
    "Increased complexity: Polynomial regression models with a high degree of polynomial terms can be complex and \n",
    "difficult to interpret, especially when compared to linear regression models.\n",
    "\n",
    "In situations where the relationship between the predictor variable and the outcome variable is non-linear,\n",
    "polynomial regression may be preferred over linear regression. For example, if the relationship between age\n",
    "and salary is curved, linear regression may not capture this relationship accurately, while a polynomial\n",
    "regression model with a quadratic term can be used to capture the curve. However, it is important to be \n",
    "cautious when using polynomial regression and avoid overfitting by selecting an appropriate degree of polynomial\n",
    "terms and validating the model using holdout data or cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
